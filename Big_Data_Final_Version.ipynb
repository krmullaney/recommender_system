{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcKQ3J9pwqpT"
      },
      "source": [
        "---\n",
        "\n",
        "**<center>Installations and Imports**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsMfrwbYwDqw",
        "outputId": "0a373281-8b1f-4afc-979d-c30780412401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=a1fa1cbb16c8ef0e9bb3d0ffb78d64cb6bef8f04f0921e9fa9411735b1d9aa4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lenskit\n",
            "  Downloading lenskit-0.14.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seedbank>=0.1.0\n",
            "  Downloading seedbank-0.1.2-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from lenskit) (1.21.6)\n",
            "Requirement already satisfied: pandas==1.*,>=1.0 in /usr/local/lib/python3.8/dist-packages (from lenskit) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.8/dist-packages (from lenskit) (1.7.3)\n",
            "Requirement already satisfied: cffi>=1.12.2 in /usr/local/lib/python3.8/dist-packages (from lenskit) (1.15.1)\n",
            "Requirement already satisfied: numba<0.57,>=0.51 in /usr/local/lib/python3.8/dist-packages (from lenskit) (0.56.4)\n",
            "Collecting csr>=0.3.1\n",
            "  Downloading csr-0.4.3-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.8/dist-packages (from lenskit) (5.4.8)\n",
            "Collecting binpickle>=0.3.2\n",
            "  Downloading binpickle-0.3.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.*,>=1.0->lenskit) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.*,>=1.0->lenskit) (2.8.2)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.8/dist-packages (from binpickle>=0.3.2->lenskit) (1.0.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12.2->lenskit) (2.21)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba<0.57,>=0.51->lenskit) (6.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba<0.57,>=0.51->lenskit) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba<0.57,>=0.51->lenskit) (0.39.1)\n",
            "Collecting anyconfig\n",
            "  Downloading anyconfig-0.13.0-py2.py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.8/87.8 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas==1.*,>=1.0->lenskit) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba<0.57,>=0.51->lenskit) (3.11.0)\n",
            "Installing collected packages: binpickle, anyconfig, seedbank, csr, lenskit\n",
            "Successfully installed anyconfig-0.13.0 binpickle-0.3.4 csr-0.4.3 lenskit-0.14.2 seedbank-0.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lightfm\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.1/310.1 KB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from lightfm) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from lightfm) (1.7.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from lightfm) (2.25.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from lightfm) (1.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->lightfm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->lightfm) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->lightfm) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->lightfm) (4.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->lightfm) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->lightfm) (3.1.0)\n",
            "Building wheels for collected packages: lightfm\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp38-cp38-linux_x86_64.whl size=916414 sha256=6565f005ee38089863ea2c0e34290c5080c4a2a74a5d3d1192a798c7ffce2587\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/bb/51/9c487d021c1373b691d13cadca0b65b6852627b1f3f43550fa\n",
            "Successfully built lightfm\n",
            "Installing collected packages: lightfm\n",
            "Successfully installed lightfm-1.16\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j\n",
        "!pip install lenskit\n",
        "!pip install lightfm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "s8DeAmRiwTZS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array, avg, coalesce, col, collect_list, count, flatten, lit, monotonically_increasing_id\n",
        "from pyspark.mllib.evaluation import RankingMetrics\n",
        "from pyspark.ml.evaluation import RankingEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "import pandas as pd \n",
        "\n",
        "#from lenskit import batch, topn, util, topn\n",
        "#from lenskit import crossfold as xf\n",
        "#from lenskit.algorithms import Recommender, als as als_slow, item_knn as knn\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from scipy import sparse\n",
        "from numpy import array as np_arr\n",
        "import numpy as np\n",
        "from lightfm import LightFM\n",
        "from lightfm.evaluation import precision_at_k\n",
        "\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biAF06gIx9-C"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center>Manually Set User-Specific File Paths <center>** \n",
        "\n",
        "--- \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CMTIbbNDyGYp"
      },
      "outputs": [],
      "source": [
        "#download and save both dataset folders('ml_latest_small' and 'ml_latest') to a Google Drive.  \n",
        "#set MOUNT POINT/ROOT to point to the folder where both files are saved.\n",
        "MOUNT_POINT = '/content/drive'\n",
        "ROOT = f'{MOUNT_POINT}/MyDrive/Big Data Final Project/'\n",
        "\n",
        "#to work with the small file, set SMALL to True. To work with the large file, set small to False.\n",
        "SMALL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bcIuZQ-o5e9a"
      },
      "outputs": [],
      "source": [
        "#sets ML_PATH to the the path of either the small or large file\n",
        "ML_DIR = 'ml_latest' if not SMALL else 'ml_latest_small'\n",
        "ML_PATH = f'{ROOT}/{ML_DIR}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpLZbIl_6ms3"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Finalize Set Up <center>** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgYAkodKDfw1",
        "outputId": "54c6863f-32cd-4117-b796-f3182a6d64eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount drive\n",
        "drive.mount(MOUNT_POINT)\n",
        "\n",
        "#start spark session\n",
        "spark = SparkSession.builder.appName('Recommender').getOrCreate()\n",
        "\n",
        "#take first spark checkpoint\n",
        "spark.sparkContext.setCheckpointDir(ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "USZr7CfH64Wm"
      },
      "outputs": [],
      "source": [
        "#download ratings file\n",
        "RATINGS_SCHEMA = 'user INT, product INT, rating FLOAT, timestamp INT'\n",
        "ratings = spark.read.csv(f'{ML_PATH}/ratings.csv', schema=RATINGS_SCHEMA).dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV9LtpDY7H8i"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Split The Ratings File <center>** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yr-ossAN7eZp"
      },
      "outputs": [],
      "source": [
        "#function that takes in a ratings dataframe, groups ratings by user ID, then takes a random sample corresponding to a set fraction of each users ratings\n",
        "#the function then takes a checkpoint to prevent future unnecessary recalculations\n",
        "def sample(df, frac):\n",
        "  return df.groupBy('user').applyInPandas(lambda group: group.sample(frac=frac), RATINGS_SCHEMA).checkpoint()\n",
        "\n",
        "#function to trim the ratings that were randomly sampled from the original ratings dataframe\n",
        "def exclude(df1, df2):\n",
        "  return df1.subtract(df2)\n",
        "\n",
        "#function that checks to see if a 70-15-15 train-val-test split of the ratings file already exists in the cached folder. If not, it executes a split.\n",
        "#set overwrite to True if you want a fresh split no matter what\n",
        "cache_path = f'{ROOT}/cached'\n",
        "def split(ratings, overwrite=False):\n",
        "  size = '-small' if SMALL else '-large'\n",
        "  trainingPath = f'{cache_path}/training{size}.parquet'\n",
        "  validationPath = f'{cache_path}/validation{size}.parquet'\n",
        "  testPath = f'{cache_path}/test{size}.parquet'\n",
        "\n",
        "  if (overwrite or not os.path.exists(trainingPath) or not os.path.exists(validationPath) or not os.path.exists(testPath)):\n",
        "    nontraining = sample(ratings, 0.3)\n",
        "\n",
        "    training = exclude(ratings, nontraining)\n",
        "    test = sample(nontraining, 0.5)\n",
        "    validation = exclude(nontraining, test)\n",
        "\n",
        "    training.write.parquet(trainingPath, 'overwrite')\n",
        "    validation.write.parquet(validationPath, 'overwrite')\n",
        "    test.write.parquet(testPath, 'overwrite')\n",
        "  else:\n",
        "    training = spark.read.parquet(trainingPath)\n",
        "    validation = spark.read.parquet(validationPath)\n",
        "    test = spark.read.parquet(testPath)\n",
        "\n",
        "  return training, validation, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8iVPqB49iE4x"
      },
      "outputs": [],
      "source": [
        "#execute a split on the ratings file\n",
        "training, validation, test = split(ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-cLOZaobSMOb"
      },
      "outputs": [],
      "source": [
        "train_size = training.count()\n",
        "val_size = validation.count()\n",
        "test_size = validation.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk5mGJhziFQl",
        "outputId": "879844d3-4363-487c-d4ae-7ef90f3d6ee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the training dataset: 70595\n",
            "Number of rows in the validation dataset: 15116\n",
            "Number of rows in the testing dataset: 15116\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of rows in the training dataset:\" , train_size)\n",
        "print(\"Number of rows in the validation dataset:\", val_size)\n",
        "print(\"Number of rows in the testing dataset:\", test_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuvUFjP3iLaq",
        "outputId": "5bc6ed5b-0096-4d7c-82b0-32f68fdd1f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70.01 % of the original dataset is in training\n",
            "14.99 % of the original dataset is in validation\n",
            "14.99 % of the original dataset is in testing\n"
          ]
        }
      ],
      "source": [
        "orig_total_rows = ratings.count()\n",
        "print(round(train_size/orig_total_rows*100,2), \"% of the original dataset is in training\")\n",
        "print(round(val_size/orig_total_rows*100,2), \"% of the original dataset is in validation\")\n",
        "print(round(test_size/orig_total_rows*100,2), \"% of the original dataset is in testing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-bESiKXmY6q"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Create a Model Class<center>** \n",
        "\n",
        "--- \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t9mIPOVTT26H"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "  model = None #stores the trained model\n",
        "  params = {} #stores a model's hyperparamaters\n",
        "  n_recs = 100 #set the default number of recommendations for each model to 100\n",
        "  \n",
        "  def __init__(self, train, val, test):\n",
        "    self.train = train\n",
        "    self.val = val\n",
        "    self.test = test\n",
        "  \n",
        "  #build specifies the functionality behind how a model should make recommendations \n",
        "  def build(self):\n",
        "    pass\n",
        "\n",
        "  #recommend applies the model's functionality to a set of users that we want to make recommendations for \n",
        "  def recommend(self, dataset):\n",
        "    pass\n",
        "\n",
        "  #scores the model using ranking metrics - precisionAtK, meanAveragePrecisionAtK, ndcgAtK, meanAveragePrecision\n",
        "  def score(self, test=False):\n",
        "    dataset = self.test if test else self.val\n",
        "    rec_rel = self.recommend(dataset).select(col('recommended').cast('array<double>'), col('relevant').cast('array<double>'))\n",
        "    \n",
        "    evaluator = RankingEvaluator(predictionCol='recommended', labelCol='relevant')\n",
        "    \n",
        "    metrics = (\n",
        "        evaluator.evaluate(rec_rel, { evaluator.metricName: 'precisionAtK', evaluator.k: self.n_recs}),\n",
        "        evaluator.evaluate(rec_rel, { evaluator.metricName: 'meanAveragePrecisionAtK', evaluator.k: self.n_recs}),\n",
        "        evaluator.evaluate(rec_rel, { evaluator.metricName: 'ndcgAtK', evaluator.k: self.n_recs}),\n",
        "        evaluator.evaluate(rec_rel, { evaluator.metricName: 'meanAveragePrecision'}),\n",
        "    )\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0A6soSO85_F"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Create a Tuning Function** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "G_N6NwFC8BfJ"
      },
      "outputs": [],
      "source": [
        "def tuner(model, param_name, values_to_try):\n",
        "\n",
        "  #initialize lists that will store the score values as we try each new parameter value\n",
        "  p_at_ks = []\n",
        "  map_at_ks = [] \n",
        "  ndcg_at_ks = []\n",
        "  maps = []\n",
        "  \n",
        "  for value in values_to_try:\n",
        "    model.params = { **model.params, param_name: value }\n",
        "    model.build()\n",
        "    scores = model.score()\n",
        "\n",
        "    p_at_ks.append(scores[0])\n",
        "    map_at_ks.append(scores[1])\n",
        "    ndcg_at_ks.append(scores[2])\n",
        "    maps.append(scores[3])\n",
        "\n",
        "  norm_p_at_ks = list(map(lambda i: i/max(p_at_ks), p_at_ks))\n",
        "  norm_map_at_ks = list(map(lambda i: i/max(map_at_ks), map_at_ks))\n",
        "  norm_ndcg_at_ks = list(map(lambda i: i/max(ndcg_at_ks), ndcg_at_ks))\n",
        "  norm_maps = list(map(lambda i: i/max(maps), maps))\n",
        "\n",
        "  norm_scores = list(map(lambda i: round((norm_p_at_ks[i] + norm_map_at_ks[i] + norm_ndcg_at_ks[i] + norm_maps[i]) / 4,4), range(len(values_to_try))))\n",
        "\n",
        "  i = norm_scores.index(max(norm_scores))\n",
        "\n",
        "  best_value = values_to_try[i]\n",
        "  scores_with_best_value = p_at_ks[i], map_at_ks[i], ndcg_at_ks[i],maps[i]\n",
        "\n",
        "  return best_value, scores_with_best_value, norm_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGCrBi4HcLGV"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Create a Popularity Model Class** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NM_-qxj3cS7k"
      },
      "outputs": [],
      "source": [
        "class PopularityModel(Model):\n",
        "  params = { 'min_rating': 0 }\n",
        "\n",
        "  def build(self):\n",
        "    \"Creates a list containing the 100 most popular movies. Popular movies here are those with the most views that maintained an average minimum ratings above 'min_rating'.\"\n",
        "    popular_movies = self.train.groupBy('product').agg({'rating': 'mean', 'product': 'count'}).filter(col('avg(rating)') >= self.params['min_rating']).sort('count(product)', ascending=False).limit(self.n_recs).select('product').rdd.flatMap(lambda x: x)  \n",
        "    self.model = popular_movies\n",
        "\n",
        "  def recommend(self, dataset):\n",
        "    \"Returns an RRD with two lists for each user. One list contains the movie ID's that were recommended to that user. The other contains the movie ID's that the user viewed and rated above a 3.0.\"\n",
        "    recommended = self.model.collect()\n",
        "    relevant = dataset.filter(col('rating') > 3.0).groupBy('user').agg(collect_list('product').alias('relevant'))\n",
        "    rec_rel = relevant.withColumn('recommended', array(list(map(lit, recommended)))).select('recommended', 'relevant')\n",
        "    return rec_rel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY13xWaEYHYm"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Find the Optimal Cutoff for our Popularity Model** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeeWH16q-aIS",
        "outputId": "06147892-f5e1-4fb2-c66c-27cd0e4d2a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Scores for Tuning Minimum Average Movie Rating :\n",
            "                   3.00    3.01    3.02    3.03    3.04    3.05    3.06  \\\n",
            "Min Avg Rating                                                            \n",
            "Adjusted Scores  0.9869  0.9869  0.9869  0.9868  0.9868  0.9898  0.9882   \n",
            "\n",
            "                   3.07    3.08    3.09  ...    3.90    3.91    3.92    3.93  \\\n",
            "Min Avg Rating                           ...                                   \n",
            "Adjusted Scores  0.9879  0.9892  0.9872  ...  0.9495  0.9472  0.9172  0.9099   \n",
            "\n",
            "                   3.94    3.95    3.96    3.97    3.98    3.99  \n",
            "Min Avg Rating                                                   \n",
            "Adjusted Scores  0.9097  0.9072  0.8822  0.8796  0.8655  0.8522  \n",
            "\n",
            "[1 rows x 100 columns]\n",
            "\n",
            "Best Performing Average Movie Rating Cutoff: 3.46 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#create a popularity model\n",
        "pop_model = PopularityModel(training, validation, test)\n",
        "\n",
        "#use tuner to try different values for min_rating\n",
        "cutoffs_to_try = list(map(lambda r: r/100, range(300, 400, 1)))\n",
        "best_cutoff, scores_with_cutoff, pop_norm_scores = tuner(pop_model,'min_rating', cutoffs_to_try)\n",
        "pop_scores_df = pd.DataFrame([pop_norm_scores], columns=cutoffs_to_try, index=[\"Adjusted Scores\"])\n",
        "pop_scores_df.index.name = 'Min Avg Rating'\n",
        "\n",
        "print('Performance Scores for Tuning Minimum Average Movie Rating :')\n",
        "print(pop_scores_df)\n",
        "print()\n",
        "print('Best Performing Average Movie Rating Cutoff:', best_cutoff,'\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s3GHItaCGli"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Final Results of the Poplarity Model on The Validation Set** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QQilHhjlWD0t"
      },
      "outputs": [],
      "source": [
        "def pretty_print_metrics(metrics):\n",
        "  print('Precision at K', round(metrics[0], 4))\n",
        "  print('MAP at K', round(metrics[1], 4))\n",
        "  print('NDCG at K', round(metrics[2], 4))\n",
        "  print('MAP', round(metrics[3], 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuZaUBNZCWRO",
        "outputId": "bed34153-0985-4860-92e4-aea0683b5829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on Validation Set\n",
            "Precision at K 0.0304\n",
            "MAP at K 0.0398\n",
            "NDCG at K 0.1464\n",
            "MAP 0.0398\n"
          ]
        }
      ],
      "source": [
        "print('Results on Validation Set')\n",
        "pretty_print_metrics(scores_with_cutoff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgDq9jJ_CSaZ"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Final Results of the Poplarity Model on The Test Set** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDfZkjkUL0ci",
        "outputId": "fefd0774-a3ae-42f8-d488-0e5e2ba60b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on Test Set\n",
            "Precision at K 0.0308\n",
            "MAP at K 0.036\n",
            "NDCG at K 0.1423\n",
            "MAP 0.0359\n"
          ]
        }
      ],
      "source": [
        "pop_model.params = {'min_rating': best_cutoff}\n",
        "pop_model.build()\n",
        "test_scores = pop_model.score(True)\n",
        "\n",
        "print('Results on Test Set')\n",
        "pretty_print_metrics(test_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB1C4F_26SO5"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Create a Latent Factor Model Class** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3AUL47mRSG2I"
      },
      "outputs": [],
      "source": [
        "class LFModel(Model):\n",
        "  params = {\n",
        "      'Num Iterations': 3,\n",
        "      'Rank': 3,\n",
        "      'Reg Param': 0.1,\n",
        "      'Min Count': 50\n",
        "  }\n",
        "\n",
        "  def build(self):\n",
        "    counts = self.train.groupBy('product').agg({'product': 'count'})\n",
        "    train = self.train.join(counts, on='product', how='left').where(col('count(product)') >= self.params['Min Count']).drop('timestamp').drop('count(product)').select('user', 'product', 'rating').withColumnRenamed('product', 'item')\n",
        "\n",
        "    als = ALS(rank=self.params['Rank'], maxIter=self.params['Num Iterations'], regParam=self.params['Reg Param'])\n",
        "    start = timer()\n",
        "    model = als.fit(train)\n",
        "    end = timer()\n",
        "    self.last_build_time = end - start\n",
        "    self.model = model\n",
        "  \n",
        "  def recommend(self, dataset):\n",
        "    recommended = self.model.recommendForAllUsers(Model.n_recs)\n",
        "    relevant = dataset.where(col('rating') > 3.0).groupBy('user').agg(collect_list('product').alias('relevant'))\n",
        "    rec_rel = recommended.join(relevant, on='user', how='left').select(col('recommendations.item').alias('recommended'), 'relevant').withColumn('relevant', coalesce(col('relevant'), array()))\n",
        "\n",
        "    return rec_rel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvlzc0mCqfx"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Find the Optimal Parameters for our Latent Factor Model** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwJqcJ5Ctos4"
      },
      "source": [
        "To find the optimal parameters, default values are randomly assigned and the tuning function is used to tune each parameter one at a time. Once the best performing value for a parameter has been found, it will be set as the new default value, and the next parameter will be tuned. The process of tuning each of the four parameters in sequence, will be referred to in totality as a complete tuning iteration. \n",
        "\n",
        "This process is implemented to avoid having to run an unreasonable number of parameter variations on an already large model. Because not every possible parameter variation can be run, several techniques are implemented to reinforce that the parameters located are likely optimal. \n",
        "\n",
        "First, the model will continually tune parameter by parameter until two complete iterations are in total agreement about what the optimal parameters are. \n",
        "\n",
        "Second, to account for the fact that the previous and current iterations might be more inclined to agree (as the results of the first iterations were used as the starting parameter settings for the second iteration), the entire tuning process will be run again with different default parameter values and with a different parameter tuning order. If both rounds are in agreement about the optimal parameter values, the tuning process is complete and final metrics on the validation and test sets will be collected. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0oUn29Z7aEG"
      },
      "source": [
        "Round One"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDcdfxkLtnAe",
        "outputId": "802799b0-8d6c-498a-f747-fb65265e093d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Rank\n",
            "Current Num Iterations: 5.00\n",
            "Current Min Count: 60.00\n",
            "Current Reg Param: 0.15\n",
            "\n",
            "Performance Scores for Tuning Rank :\n",
            "                    5    10      50      100     200\n",
            "Rank                                                \n",
            "Adjusted Scores  0.9721  1.0  0.9575  0.9351  0.9259\n",
            "\n",
            "Best Performing Rank: 10\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0325\n",
            "MAP at K 0.041\n",
            "NDCG at K 0.1539\n",
            "MAP 0.041\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Num Iterations\n",
            "Current Rank: 10.00\n",
            "Current Min Count: 60.00\n",
            "Current Reg Param: 0.15\n",
            "\n",
            "Performance Scores for Tuning Num Iterations :\n",
            "                     1    5       10      15      20\n",
            "Num Iterations                                      \n",
            "Adjusted Scores  0.6177  1.0  0.9196  0.9181  0.9207\n",
            "\n",
            "Best Performing Num Iterations: 5\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0325\n",
            "MAP at K 0.041\n",
            "NDCG at K 0.1539\n",
            "MAP 0.041\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Min Count\n",
            "Current Rank: 10.00\n",
            "Current Num Iterations: 5.00\n",
            "Current Reg Param: 0.15\n",
            "\n",
            "Performance Scores for Tuning Min Count :\n",
            "                 60      70      80     90      100\n",
            "Min Count                                          \n",
            "Adjusted Scores  1.0  0.9765  0.9708  0.829  0.7392\n",
            "\n",
            "Best Performing Min Count: 60\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0325\n",
            "MAP at K 0.041\n",
            "NDCG at K 0.1539\n",
            "MAP 0.041\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Reg Param\n",
            "Current Rank: 10.00\n",
            "Current Num Iterations: 5.00\n",
            "Current Min Count: 60.00\n",
            "\n",
            "Performance Scores for Tuning Reg Param :\n",
            "                   0.15    0.20    0.25    0.30   0.35\n",
            "Reg Param                                             \n",
            "Adjusted Scores  0.9857  0.9904  0.9341  0.8986  0.858\n",
            "\n",
            "Best Performing Reg Param: 0.2\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Iteration Complete\n",
            "\n",
            "Previous Iteration Optimal Parameters\n",
            "Rank: 1  Num Iterations: 5  Min Count: 60  Reg Param: 0.15\n",
            "Current Iteration Optimal Parameters\n",
            "Rank: 10  Num Iterations: 5  Min Count: 60  Reg Param: 0.2\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Rank\n",
            "Current Num Iterations: 5.00\n",
            "Current Min Count: 60.00\n",
            "Current Reg Param: 0.20\n",
            "\n",
            "Performance Scores for Tuning Rank :\n",
            "                    5    10      50      100     200\n",
            "Rank                                                \n",
            "Adjusted Scores  0.9405  1.0  0.9735  0.9537  0.9597\n",
            "\n",
            "Best Performing Rank: 10\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Num Iterations\n",
            "Current Rank: 10.00\n",
            "Current Min Count: 60.00\n",
            "Current Reg Param: 0.20\n",
            "\n",
            "Performance Scores for Tuning Num Iterations :\n",
            "                     1    5       10      15     20\n",
            "Num Iterations                                     \n",
            "Adjusted Scores  0.6167  1.0  0.9252  0.9167  0.919\n",
            "\n",
            "Best Performing Num Iterations: 5\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Min Count\n",
            "Current Rank: 10.00\n",
            "Current Num Iterations: 5.00\n",
            "Current Reg Param: 0.20\n",
            "\n",
            "Performance Scores for Tuning Min Count :\n",
            "                 60      70      80      90     100\n",
            "Min Count                                          \n",
            "Adjusted Scores  1.0  0.9873  0.9705  0.8592  0.745\n",
            "\n",
            "Best Performing Min Count: 60\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Reg Param\n",
            "Current Rank: 10.00\n",
            "Current Num Iterations: 5.00\n",
            "Current Min Count: 60.00\n",
            "\n",
            "Performance Scores for Tuning Reg Param :\n",
            "                   0.15    0.20    0.25    0.30   0.35\n",
            "Reg Param                                             \n",
            "Adjusted Scores  0.9857  0.9904  0.9341  0.8986  0.858\n",
            "\n",
            "Best Performing Reg Param: 0.2\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Iteration Complete\n",
            "\n",
            "Previous Iteration Optimal Parameters\n",
            "Rank: 10  Num Iterations: 5  Min Count: 60  Reg Param: 0.2\n",
            "Current Iteration Optimal Parameters\n",
            "Rank: 10  Num Iterations: 5  Min Count: 60  Reg Param: 0.2\n"
          ]
        }
      ],
      "source": [
        "#create a latent factor model \n",
        "latent = LFModel(training, validation, test)\n",
        "\n",
        "#use tuner to try different values for each parameter\n",
        "tries = {\n",
        "    'Rank': [5, 10, 50, 100, 200],\n",
        "    'Num Iterations': [1, 5, 10, 15, 20],\n",
        "    'Min Count': [60, 70, 80, 90, 100],\n",
        "    'Reg Param': [0.15, 0.20, 0.25, 0.30, 0.35],\n",
        "}\n",
        "\n",
        "latent.params['Num Iterations'] = 5\n",
        "latent.params['Rank'] = 1\n",
        "latent.params['Min Count'] = 60\n",
        "latent.params['Reg Param'] = 0.15\n",
        "\n",
        "param_names = [name for name in tries]\n",
        "\n",
        "previous_results = []\n",
        "\n",
        "for param_name in tries:\n",
        "  previous_results.append(LFModel.params[param_name])\n",
        "\n",
        "while True:\n",
        "  results = []\n",
        "\n",
        "  for param_name in tries:\n",
        "    best, scores, norm_scores = tuner(latent, param_name, tries[param_name])\n",
        "\n",
        "    scores_df = pd.DataFrame([norm_scores], columns=tries[param_name], index=[\"Adjusted Scores\"])\n",
        "    scores_df.index.name = param_name\n",
        "\n",
        "    print('\\n---------------------------------------------------------\\n') \n",
        "    print('Tuning Parameter:', param_name)\n",
        "    for name in param_names:\n",
        "      if name == param_name:\n",
        "        continue\n",
        "      print('Current %s: %.2f' % (name, latent.params[name]))\n",
        "    print('\\nPerformance Scores for Tuning',param_name,':')\n",
        "    print(scores_df)\n",
        "    print(f'\\nBest Performing {param_name}: {best}\\n')\n",
        "    print('Results on Validation Set:')\n",
        "    print('Precision at K', round(scores[0],4))\n",
        "    print('MAP at K', round(scores[1],4))\n",
        "    print('NDCG at K', round(scores[2],4))\n",
        "    print('MAP', round(scores[3],4))\n",
        "    latent.params[param_name] = best\n",
        "    results.append(best)\n",
        "  \n",
        "  print('\\n---------------------------------------------------------\\n')\n",
        "  print('Iteration Complete')\n",
        "  print('\\nPrevious Iteration Optimal Parameters')\n",
        "  print('Rank:', previous_results[0],' Num Iterations:', previous_results[1],' Min Count:', previous_results[2],' Reg Param:', previous_results[3])\n",
        "  print('Current Iteration Optimal Parameters')\n",
        "  print('Rank:', results[0],' Num Iterations:', results[1],' Min Count:', results[2],' Reg Param:', results[3])\n",
        "\n",
        "  done = True\n",
        "  for i in range(len(results)):\n",
        "    if results[i] != previous_results[i]:\n",
        "      done = False\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "  previous_results = results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUFI6YAS7n0P"
      },
      "source": [
        "Round Two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBV5rE1yAnF1",
        "outputId": "0a7065c5-4481-4ba3-a42d-3071de20c1fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Reg Param\n",
            "Current Min Count: 100.00\n",
            "Current Num Iterations: 20.00\n",
            "Current Rank: 200.00\n",
            "\n",
            "Performance Scores for Tuning Reg Param :\n",
            "                   0.15    0.20    0.25  0.30    0.35\n",
            "Reg Param                                            \n",
            "Adjusted Scores  0.8985  0.9467  0.9834   1.0  0.9959\n",
            "\n",
            "Best Performing Reg Param: 0.3\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0194\n",
            "MAP at K 0.0378\n",
            "NDCG at K 0.1193\n",
            "MAP 0.0378\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Min Count\n",
            "Current Reg Param: 0.35\n",
            "Current Num Iterations: 20.00\n",
            "Current Rank: 200.00\n",
            "\n",
            "Performance Scores for Tuning Min Count :\n",
            "                    60      70      80      90      100\n",
            "Min Count                                              \n",
            "Adjusted Scores  0.9208  0.9698  0.9966  0.9392  0.8613\n",
            "\n",
            "Best Performing Min Count: 80\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.03\n",
            "MAP at K 0.0382\n",
            "NDCG at K 0.1432\n",
            "MAP 0.0382\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Num Iterations\n",
            "Current Reg Param: 0.35\n",
            "Current Min Count: 100.00\n",
            "Current Rank: 200.00\n",
            "\n",
            "Performance Scores for Tuning Num Iterations :\n",
            "                     1    5       10      15      20\n",
            "Num Iterations                                      \n",
            "Adjusted Scores  0.7629  1.0  0.9765  0.9767  0.9765\n",
            "\n",
            "Best Performing Num Iterations: 5\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0301\n",
            "MAP at K 0.0398\n",
            "NDCG at K 0.1453\n",
            "MAP 0.0398\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Rank\n",
            "Current Reg Param: 0.35\n",
            "Current Min Count: 100.00\n",
            "Current Num Iterations: 20.00\n",
            "\n",
            "Performance Scores for Tuning Rank :\n",
            "                    5    10      50      100     200\n",
            "Rank                                                \n",
            "Adjusted Scores  0.9802  1.0  0.9969  0.9946  0.9939\n",
            "\n",
            "Best Performing Rank: 10\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0301\n",
            "MAP at K 0.0402\n",
            "NDCG at K 0.1457\n",
            "MAP 0.0402\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Iteration Complete\n",
            "\n",
            "Previous Iteration Optimal Parameters\n",
            "Rank: 200  Num Iterations: 20  Min Count: 100  Reg Param: 0.35\n",
            "Current Iteration Optimal Parameters\n",
            "Rank: 10  Num Iterations: 5  Min Count: 80  Reg Param: 0.3\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Reg Param\n",
            "Current Min Count: 100.00\n",
            "Current Num Iterations: 20.00\n",
            "Current Rank: 200.00\n",
            "\n",
            "Performance Scores for Tuning Reg Param :\n",
            "                  0.15    0.20    0.25    0.30    0.35\n",
            "Reg Param                                             \n",
            "Adjusted Scores  0.994  0.9987  0.9892  0.9815  0.9482\n",
            "\n",
            "Best Performing Reg Param: 0.2\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0301\n",
            "MAP at K 0.0413\n",
            "NDCG at K 0.1482\n",
            "MAP 0.0413\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Min Count\n",
            "Current Reg Param: 0.35\n",
            "Current Num Iterations: 20.00\n",
            "Current Rank: 200.00\n",
            "\n",
            "Performance Scores for Tuning Min Count :\n",
            "                 60      70      80      90     100\n",
            "Min Count                                          \n",
            "Adjusted Scores  1.0  0.9873  0.9705  0.8592  0.745\n",
            "\n",
            "Best Performing Min Count: 60\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Num Iterations\n",
            "Current Reg Param: 0.35\n",
            "Current Min Count: 100.00\n",
            "Current Rank: 200.00\n",
            "\n",
            "Performance Scores for Tuning Num Iterations :\n",
            "                     1    5       10      15     20\n",
            "Num Iterations                                     \n",
            "Adjusted Scores  0.6167  1.0  0.9252  0.9167  0.919\n",
            "\n",
            "Best Performing Num Iterations: 5\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Rank\n",
            "Current Reg Param: 0.35\n",
            "Current Min Count: 100.00\n",
            "Current Num Iterations: 20.00\n",
            "\n",
            "Performance Scores for Tuning Rank :\n",
            "                    5    10      50      100     200\n",
            "Rank                                                \n",
            "Adjusted Scores  0.9405  1.0  0.9735  0.9537  0.9597\n",
            "\n",
            "Best Performing Rank: 10\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Iteration Complete\n",
            "\n",
            "Previous Iteration Optimal Parameters\n",
            "Rank: 10  Num Iterations: 5  Min Count: 80  Reg Param: 0.3\n",
            "Current Iteration Optimal Parameters\n",
            "Rank: 10  Num Iterations: 5  Min Count: 60  Reg Param: 0.2\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Reg Param\n",
            "Current Min Count: 100.00\n",
            "Current Num Iterations: 20.00\n",
            "Current Rank: 200.00\n",
            "\n",
            "Performance Scores for Tuning Reg Param :\n",
            "                   0.15    0.20    0.25    0.30   0.35\n",
            "Reg Param                                             \n",
            "Adjusted Scores  0.9857  0.9904  0.9341  0.8986  0.858\n",
            "\n",
            "Best Performing Reg Param: 0.2\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Min Count\n",
            "Current Reg Param: 0.35\n",
            "Current Num Iterations: 20.00\n",
            "Current Rank: 200.00\n",
            "\n",
            "Performance Scores for Tuning Min Count :\n",
            "                 60      70      80      90     100\n",
            "Min Count                                          \n",
            "Adjusted Scores  1.0  0.9873  0.9705  0.8592  0.745\n",
            "\n",
            "Best Performing Min Count: 60\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Num Iterations\n",
            "Current Reg Param: 0.35\n",
            "Current Min Count: 100.00\n",
            "Current Rank: 200.00\n",
            "\n",
            "Performance Scores for Tuning Num Iterations :\n",
            "                     1    5       10      15     20\n",
            "Num Iterations                                     \n",
            "Adjusted Scores  0.6167  1.0  0.9252  0.9167  0.919\n",
            "\n",
            "Best Performing Num Iterations: 5\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Tuning Parameter: Rank\n",
            "Current Reg Param: 0.35\n",
            "Current Min Count: 100.00\n",
            "Current Num Iterations: 20.00\n",
            "\n",
            "Performance Scores for Tuning Rank :\n",
            "                    5    10      50      100     200\n",
            "Rank                                                \n",
            "Adjusted Scores  0.9405  1.0  0.9735  0.9537  0.9597\n",
            "\n",
            "Best Performing Rank: 10\n",
            "\n",
            "Results on Validation Set:\n",
            "Precision at K 0.0316\n",
            "MAP at K 0.0422\n",
            "NDCG at K 0.1522\n",
            "MAP 0.0422\n",
            "\n",
            "---------------------------------------------------------\n",
            "\n",
            "Iteration Complete\n",
            "\n",
            "Previous Iteration Optimal Parameters\n",
            "Rank: 10  Num Iterations: 5  Min Count: 60  Reg Param: 0.2\n",
            "Current Iteration Optimal Parameters\n",
            "Rank: 10  Num Iterations: 5  Min Count: 60  Reg Param: 0.2\n"
          ]
        }
      ],
      "source": [
        "#create a latent factor model \n",
        "latent = LFModel(training, validation, test)\n",
        "\n",
        "#use tuner to try different values for each parameter\n",
        "tries = {\n",
        "    'Reg Param': [0.15, 0.20, 0.25, 0.30, 0.35],\n",
        "    'Min Count': [60, 70, 80, 90, 100],\n",
        "    'Num Iterations': [1, 5, 10, 15, 20],\n",
        "    'Rank': [5, 10, 50, 100, 200],\n",
        "}\n",
        "\n",
        "latent.params['Num Iterations'] = 20\n",
        "latent.params['Rank'] = 200\n",
        "latent.params['Min Count'] = 100\n",
        "latent.params['Reg Param'] = 0.35\n",
        "\n",
        "param_names = [name for name in tries]\n",
        "\n",
        "previous_results = []\n",
        "\n",
        "for param_name in tries:\n",
        "  previous_results.append(latent.params[param_name])\n",
        "\n",
        "while True:\n",
        "  results = []\n",
        "\n",
        "  for param_name in tries:\n",
        "    best, scores, norm_scores = tuner(latent, param_name, tries[param_name])\n",
        "\n",
        "    scores_df = pd.DataFrame([norm_scores], columns=tries[param_name], index=[\"Adjusted Scores\"])\n",
        "    scores_df.index.name = param_name\n",
        "\n",
        "    print('\\n---------------------------------------------------------\\n') \n",
        "    print('Tuning Parameter:', param_name)\n",
        "    for name in param_names:\n",
        "      if name == param_name:\n",
        "        continue\n",
        "      print('Current %s: %.2f' % (name, LFModel.params[name]))\n",
        "    print('\\nPerformance Scores for Tuning',param_name,':')\n",
        "    print(scores_df)\n",
        "    print(f'\\nBest Performing {param_name}: {best}\\n')\n",
        "    print('Results on Validation Set:')\n",
        "    pretty_print_metrics(scores)\n",
        "    latent.params[param_name] = best\n",
        "    results.append(best)\n",
        "  \n",
        "  print('\\n---------------------------------------------------------\\n')\n",
        "  print('Iteration Complete')\n",
        "  print('\\nPrevious Iteration Optimal Parameters')\n",
        "  print('Rank:', previous_results[3],' Num Iterations:', previous_results[2],' Min Count:', previous_results[1],' Reg Param:', previous_results[0])\n",
        "  print('Current Iteration Optimal Parameters')\n",
        "  print('Rank:', results[3],' Num Iterations:', results[2],' Min Count:', results[1],' Reg Param:', results[0])\n",
        "\n",
        "  done = True\n",
        "  for i in range(len(results)):\n",
        "    if results[i] != previous_results[i]:\n",
        "      done = False\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "\n",
        "  previous_results = results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrwLmUERP9uk"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Final Results of the Latent Factor Model on The Validation Set** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJQAnva7XpP6",
        "outputId": "9ac4c1bc-aa22-4980-d791-e6cd593fbaac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on Validation Set\n",
            "Precision at K 0.0315\n",
            "MAP at K 0.0396\n",
            "NDCG at K 0.1469\n",
            "MAP 0.0396\n"
          ]
        }
      ],
      "source": [
        "print('Results on Validation Set')\n",
        "pretty_print_metrics(latent.score())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnJ6ChoKQEeg"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Final Results of the Latent Factor Model on The Test Set** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT8158v4c8gE",
        "outputId": "9374617c-31e3-44b2-b966-e8c5bae863f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on Test Set\n",
            "Precision at K 0.0315\n",
            "MAP at K 0.0339\n",
            "NDCG at K 0.1394\n",
            "MAP 0.0338\n"
          ]
        }
      ],
      "source": [
        "print('Results on Test Set')\n",
        "pretty_print_metrics(latent.score(True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey5tfcOFBE4d"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> Extension: LightFM Implementation <center>** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "BhHrkyKyBTE7"
      },
      "outputs": [],
      "source": [
        "lightfm_model = LightFM()\n",
        "\n",
        "def build_sparse_matrix(input_dataset):\n",
        "  dataset_with_id = input_dataset.withColumn(\"id\", monotonically_increasing_id())\n",
        "  increment = 10**6\n",
        "  start = 0\n",
        "\n",
        "  MAT_SIZE = (283229, 193887) if not SMALL else (611, 193610)\n",
        "  result = sparse.coo_matrix(MAT_SIZE, dtype=np.float16)\n",
        "\n",
        "  while True:\n",
        "    chunk = dataset_with_id.where(col('id').between(start, start + increment - 1))\n",
        "    if chunk.count() == 0:\n",
        "      break\n",
        "\n",
        "    I = np_arr(chunk.select('user').toPandas()['user'])\n",
        "    J = np_arr(chunk.select('product').toPandas()['product'])\n",
        "    V = np_arr(chunk.select('rating').toPandas()['rating'])\n",
        "\n",
        "    sparse_chunk = sparse.coo_matrix((V, (I, J)), shape=MAT_SIZE)\n",
        "    result += sparse_chunk\n",
        "\n",
        "    start += increment\n",
        "\n",
        "  return result\n",
        "\n",
        "sparse_train = build_sparse_matrix(training)\n",
        "sparse_val = build_sparse_matrix(validation)\n",
        "sparse_test = build_sparse_matrix(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3q1expbnpHn"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> LightFM: Hyper-parameter Tuning <center>** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tune_lightfm(default_params, key, tries):\n",
        "  prec = []\n",
        "  times = []\n",
        "\n",
        "  for val in tries:\n",
        "    default_params[key] = val\n",
        "    lightfm_model = LightFM(**params)\n",
        "\n",
        "    start = timer()\n",
        "    lightfm_model.fit(sparse_train)\n",
        "    end = timer()\n",
        "\n",
        "    test_precision = precision_at_k(lightfm_model, sparse_val, k=100).mean()\n",
        "\n",
        "    prec.append(test_precision)\n",
        "    times.append(end - start)\n",
        "\n",
        "    print('LightFM Latent Factor Model')\n",
        "    print('Parameters', params)\n",
        "    print('Precision on Test Set: ', test_precision)\n",
        "    print('Time to Fit: ', end - start)\n",
        "    print()\n",
        "\n",
        "  best = tries[prec.index(max(prec))]\n",
        "  return best\n",
        "\n",
        "params = {}\n",
        "\n",
        "best_lr = tune_lightfm(params, 'learning_rate', [1, 0.1, 0.01, 0.001, 0.0001])\n",
        "params['learning_rate'] = best_lr\n",
        "print('Best Learning Rate:', best_lr)\n",
        "\n",
        "best_loss = tune_lightfm(params, 'loss', ['logistic','bpr','warp','warp-kos'])\n",
        "params['loss'] = best_loss\n",
        "print('Best Loss:', best_loss)\n",
        "\n",
        "best_no_comps = tune_lightfm(params, 'no_components', [5,10,50,100,200])\n",
        "params['no_components'] = best_no_comps\n",
        "print('Best Number of Components:', best_no_comps)\n",
        "\n",
        "best_user_alpha = tune_lightfm(params, 'user_alpha', [.01,.05,0.1,0.2,0.3])\n",
        "params['user_alpha'] = best_user_alpha\n",
        "print('Best User Alpha:', best_user_alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpdbE2yXoxVl",
        "outputId": "44400834-6888-4a50-ff5f-0a38adc4c40b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 1}\n",
            "Precision on Test Set:  0.038508195\n",
            "Time to Fit:  0.055701674998999806\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1}\n",
            "Precision on Test Set:  0.03880328\n",
            "Time to Fit:  0.058250105001206975\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.01}\n",
            "Precision on Test Set:  0.03868853\n",
            "Time to Fit:  0.05534893100048066\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.001}\n",
            "Precision on Test Set:  0.03632787\n",
            "Time to Fit:  0.05592680199697497\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.0001}\n",
            "Precision on Test Set:  0.00095081964\n",
            "Time to Fit:  0.05719663700074307\n",
            "\n",
            "Best Learning Rate: 0.1\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic'}\n",
            "Precision on Test Set:  0.038967215\n",
            "Time to Fit:  0.055015172001731116\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'bpr'}\n",
            "Precision on Test Set:  0.029852457\n",
            "Time to Fit:  0.07508795099784038\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'warp'}\n",
            "Precision on Test Set:  0.03329508\n",
            "Time to Fit:  0.09501354299936793\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'warp-kos'}\n",
            "Precision on Test Set:  0.026016392\n",
            "Time to Fit:  0.16551737199915806\n",
            "\n",
            "Best Loss: logistic\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic', 'no_components': 5}\n",
            "Precision on Test Set:  0.038885243\n",
            "Time to Fit:  0.03472809800223331\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic', 'no_components': 10}\n",
            "Precision on Test Set:  0.03891803\n",
            "Time to Fit:  0.06606060400008573\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic', 'no_components': 50}\n",
            "Precision on Test Set:  0.039065573\n",
            "Time to Fit:  0.22374570399915683\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic', 'no_components': 100}\n",
            "Precision on Test Set:  0.03904918\n",
            "Time to Fit:  0.43108473499887623\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic', 'no_components': 200}\n",
            "Precision on Test Set:  0.038885243\n",
            "Time to Fit:  0.8698656810011016\n",
            "\n",
            "Best Number of Components: 50\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic', 'no_components': 50, 'user_alpha': 0.01}\n",
            "Precision on Test Set:  0.004836065\n",
            "Time to Fit:  0.24411866100126645\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic', 'no_components': 50, 'user_alpha': 0.05}\n",
            "Precision on Test Set:  0.0066393437\n",
            "Time to Fit:  0.29275948899885407\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic', 'no_components': 50, 'user_alpha': 0.1}\n",
            "Precision on Test Set:  0.009754098\n",
            "Time to Fit:  0.36896229999911156\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic', 'no_components': 50, 'user_alpha': 0.2}\n",
            "Precision on Test Set:  0.027311476\n",
            "Time to Fit:  0.6043650179999531\n",
            "\n",
            "LightFM Latent Factor Model\n",
            "Parameters {'learning_rate': 0.1, 'loss': 'logistic', 'no_components': 50, 'user_alpha': 0.3}\n",
            "Precision on Test Set:  0.03391803\n",
            "Time to Fit:  0.8904477600008249\n",
            "\n",
            "Best User Alpha: 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBHFsG46CHAY"
      },
      "source": [
        "--- \n",
        "\n",
        "**<center> PySpark ALS: Fitting Time and Precision on Test Set  <center>** \n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "BHERWG1uB7hZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f92a267-62a6-4bba-c269-e4268f1509fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark Latent Factor Model\n",
            "Precision on Test Set:  0.0322\n",
            "Time to Fit:  3.810257482999077\n"
          ]
        }
      ],
      "source": [
        "latent = LFModel(training, validation, test)\n",
        "# latent.params = {'Num Iterations':5,\n",
        "#                  'Rank':100,\n",
        "#                  'Reg Param' :0.1,\n",
        "#                  'Min Count':100}\n",
        "latent.params = {'Num Iterations':5,\n",
        "                 'Rank':100,\n",
        "                 'Reg Param' :0.2,\n",
        "                 'Min Count':70}\n",
        "latent.build()\n",
        "test_metrics = latent.score(True)\n",
        "\n",
        "print('PySpark Latent Factor Model')\n",
        "print('Precision on Test Set: ',round(test_metrics[0],4))\n",
        "print('Time to Fit: ', latent.last_build_time)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}